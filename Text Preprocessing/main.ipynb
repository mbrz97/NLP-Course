{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "# QUESTION 1 - White Space\n",
    "\n",
    "# Define input and output filenames, and Encodings for each file\n",
    "input_filenames = [\"zahak.txt\",\"ShortSamplePersian.txt\", \"ShortSampleEnglish.txt\", \"Beanstalk.txt\"]\n",
    "output_filenames = [\"zahak1.txt\",\"ShortSamplePersian1.txt\", \"ShortSampleEnglish1.txt\", \"Beanstalk1.txt\"]\n",
    "encoding_filenames = [\"utf-8-sig\", \"utf-8-sig\", \"utf-8\", \"ANSI\"]\n",
    "\n",
    "\n",
    "# Loop over each input file and tokenize the text\n",
    "for i in range(len(input_filenames)):\n",
    "        # Load input text file\n",
    "        with open(input_filenames[i], \"r\", encoding=encoding_filenames[i]) as f:\n",
    "            input_text = f.read()\n",
    "\n",
    "        # Tokenize the input text using WhiteSpaceTokenizer\n",
    "        tokenizer = WhitespaceTokenizer()\n",
    "        tokens = tokenizer.tokenize(input_text)\n",
    "        output_text = ' '.join(tokens)\n",
    "\n",
    "        # Save the tokens to an output text file\n",
    "        with open(output_filenames[i], \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(output_text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "# QUESTION 2 - making the letters lowercase\n",
    "\n",
    "# Define input and output filenames\n",
    "input_filenames = [\"ShortSampleEnglish1.txt\", \"Beanstalk1.txt\"]\n",
    "output_filenames = [\"ShortSampleEnglish2.txt\", \"Beanstalk2.txt\"]\n",
    "\n",
    "# Loop over each input file and lowercase the text\n",
    "for i in range(len(input_filenames)):\n",
    "    # Open the input files\n",
    "    with open(input_filenames[i], \"r\", encoding='utf-8') as f:\n",
    "        # Read the contents of the files\n",
    "        text = f.read()\n",
    "\n",
    "    # Convert the contents of the files to lowercase\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # Save the lowercase contents to output files\n",
    "    with open(output_filenames[i], \"w\", encoding='utf-8') as f:\n",
    "        f.write(text_lower)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 3 - Tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "# Create a list of input file names\n",
    "input_files = [\"zahak1.txt\",\"ShortSamplePersian1.txt\", \"ShortSampleEnglish2.txt\", \"Beanstalk2.txt\"]\n",
    "output_files = [\"zahak3.txt\",\"ShortSamplePersian3.txt\", \"ShortSampleEnglish3.txt\", \"Beanstalk3.txt\"]\n",
    "encoding_filenames = [\"utf-8-sig\", \"utf-8-sig\", \"utf-8\", \"ANSI\"]\n",
    "\n",
    "# Instantiate the PunktSentenceTokenizer\n",
    "tokenizer = PunktSentenceTokenizer()\n",
    "\n",
    "# Loop over the input files and tokenize each file\n",
    "for i, file_name in enumerate(input_files):\n",
    "    # Open the input file\n",
    "    with open(file_name, \"r\", encoding=encoding_filenames[i]) as f:\n",
    "        # Read the contents of the file\n",
    "        text = f.read()\n",
    "\n",
    "        # Tokenize the text into sentences\n",
    "        sentences = tokenizer.tokenize(text)\n",
    "\n",
    "        # Write the sentences to the output file\n",
    "        with open(output_files[i], \"w\", encoding=\"utf-8\") as f_out:\n",
    "            f_out.write(\"\\n\".join(sentences))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "# QUESTION 4 - RegexpTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "input_files = [\"zahak3.txt\",\"ShortSamplePersian3.txt\", \"ShortSampleEnglish3.txt\", \"Beanstalk3.txt\"]\n",
    "output_files = [\"zahak4.txt\",\"ShortSamplePersian4.txt\", \"ShortSampleEnglish4.txt\", \"Beanstalk4.txt\"]\n",
    "\n",
    "# Only include Persian and English alphabet letters\n",
    "tokenizer = RegexpTokenizer(r'[آابپتثجچحخدذرزژسشصضطظعغفقکگلمنوهیa-zA-Z]+')\n",
    "\n",
    "\n",
    "for i in range(len(input_files)):\n",
    "\n",
    "    # Create a tokenizer using a regular expression pattern to match words\n",
    "\n",
    "    # Open the input file\n",
    "    with open(input_files[i], 'r', encoding='utf-8') as f:\n",
    "        # Read the contents of the file\n",
    "        text = f.read()\n",
    "\n",
    "        # Tokenize the text into words using the tokenizer\n",
    "        words = tokenizer.tokenize(text.lower())\n",
    "\n",
    "\n",
    "        # Write the words to the output file\n",
    "        with open(output_files[i], \"w\", encoding=\"utf-8\") as f_out:\n",
    "            f_out.write(\" \".join(words))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShortSampleEnglish4.txt Before: 80 After: 47\n",
      "Beanstalk4.txt Before: 4265 After: 2635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 5 - StopWord - English texts\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "input_files = [\"ShortSampleEnglish4.txt\", \"Beanstalk4.txt\"]\n",
    "output_files = [\"ShortSampleEnglish5.txt\", \"Beanstalk5.txt\"]\n",
    "\n",
    "# Load the stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for i in range(len(input_files)):\n",
    "    # Open the input file\n",
    "    with open(input_files[i], 'r', encoding='utf-8') as f:\n",
    "        # Read the contents of the file\n",
    "        text = f.read()\n",
    "\n",
    "        words = word_tokenize(text)\n",
    "\n",
    "        # Remove the stopwords from the list of words\n",
    "        cleaned_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "\n",
    "        # Join the cleaned words into a string with spaces between them\n",
    "        cleaned_text = ' '.join(cleaned_words)\n",
    "\n",
    "        # Write the cleaned text to an output file\n",
    "        with open(output_files[i], 'w', encoding='utf-8') as f_out:\n",
    "            f_out.write(cleaned_text)\n",
    "        print(input_files[i], 'Before:', len(text), 'After:', len(cleaned_text))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zahak4.txt Before: 5506 After: 3495\n",
      "ShortSamplePersian4.txt Before: 67 After: 26\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 5 - StopWord - Persian texts\n",
    "\n",
    "# Open the file containing Persian stopwords to be removed\n",
    "with open('persian_stopwords.txt', 'r', encoding='utf-8') as f_remove:\n",
    "    # Read the contents of the file and split them into a list of words\n",
    "    words_to_remove = f_remove.read().split()\n",
    "\n",
    "input_files = [\"zahak4.txt\",\"ShortSamplePersian4.txt\"]\n",
    "output_files = [\"zahak5.txt\",\"ShortSamplePersian5.txt\"]\n",
    "\n",
    "for i in range(len(input_files)):\n",
    "    # Open the input file\n",
    "    with open(input_files[i], 'r', encoding='utf-8') as f:\n",
    "        # Read the contents of the file\n",
    "        text = f.read()\n",
    "\n",
    "        # Split the text into a list of words\n",
    "        words = text.split()\n",
    "\n",
    "        # Remove the words from the list of words\n",
    "        cleaned_words = [word for word in words if word not in words_to_remove]\n",
    "\n",
    "        # Join the cleaned words into a string with spaces between them\n",
    "        cleaned_text = ' '.join(cleaned_words)\n",
    "\n",
    "        # Write the cleaned text to an output file\n",
    "        with open(output_files[i], 'w', encoding='utf-8') as f_out:\n",
    "            f_out.write(cleaned_text)\n",
    "\n",
    "        print(input_files[i], 'Before:', len(text), 'After:', len(cleaned_text))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "#Question 6 - Stemming\n",
    "\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "input_files = [\"ShortSampleEnglish5.txt\", \"Beanstalk5.txt\"]\n",
    "output_files = [\"ShortSampleEnglish6.txt\", \"Beanstalk6.txt\"]\n",
    "indices1 = [2]\n",
    "indices2 = [3, 11, 60, 68]\n",
    "indices = [indices1, indices2]\n",
    "\n",
    "# Create stemmers\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "for i in range(len(input_files)):\n",
    "    # Read input text file\n",
    "    with open(input_files[i], 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Select words at specific indices\n",
    "    selected_words = [words[i] for i in indices[i]]\n",
    "\n",
    "    # Stem the words using PorterStemmer and LancasterStemmer\n",
    "    porter_stems = [porter.stem(word) for word in selected_words]\n",
    "    lancaster_stems = [lancaster.stem(word) for word in selected_words]\n",
    "\n",
    "    # Save the stems to output file\n",
    "    with open(output_files[i], 'w', encoding='utf-8') as file:\n",
    "        file.write(f\"Selected words: {selected_words}\\n\")\n",
    "        file.write(f\"Porter stems: {porter_stems}\\n\")\n",
    "        file.write(f\"Lancaster stems: {lancaster_stems}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "went -> go\n",
      "better -> good\n",
      "was -> be\n",
      "eaten -> eat\n",
      "butterflies -> butterfly\n",
      "fishing -> fish\n",
      "signaling -> signal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Question 7 - Stemming\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n",
    "words = ['went', 'better', 'was', 'eaten', 'butterflies', 'fishing', 'signaling']\n",
    "pos = [VERB, ADJ, VERB, VERB, NOUN, VERB, VERB]\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    lemma = lemmatizer.lemmatize(word, pos=pos[i])\n",
    "    print(f\"{word} -> {lemma}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
